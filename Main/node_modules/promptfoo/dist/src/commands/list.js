"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.listCommand = void 0;
const chalk_1 = __importDefault(require("chalk"));
const util_1 = require("../util");
const table_1 = require("../table");
const logger_1 = __importDefault(require("../logger"));
const telemetry_1 = __importDefault(require("../telemetry"));
function listCommand(program) {
    const listCommand = program.command('list').description('List various resources');
    listCommand
        .command('evals')
        .description('List evaluations.')
        .option('--env-path <path>', 'Path to the environment file')
        .option('-n <limit>', 'Number of evals to display')
        .action(async (cmdObj) => {
        (0, util_1.setupEnv)(cmdObj.envPath);
        telemetry_1.default.maybeShowNotice();
        telemetry_1.default.record('command_used', {
            name: 'list evals',
        });
        await telemetry_1.default.send();
        const evals = await (0, util_1.getEvals)(Number(cmdObj.n) || undefined);
        const tableData = evals.map((evl) => ({
            'Eval ID': evl.id,
            Description: evl.description || '',
            Prompts: evl.results.table.head.prompts.map((p) => (0, util_1.sha256)(p.raw).slice(0, 6)).join(', '),
            Vars: evl.results.table.head.vars.map((v) => v).join(', '),
        }));
        logger_1.default.info((0, table_1.wrapTable)(tableData));
        (0, util_1.printBorder)();
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show eval <id>')} to see details of a specific evaluation.`);
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show prompt <id>')} to see details of a specific prompt.`);
    });
    listCommand
        .command('prompts')
        .description('List prompts used')
        .option('--env-path <path>', 'Path to the environment file')
        .option('-n <limit>', 'Number of prompts to display')
        .action(async (cmdObj) => {
        (0, util_1.setupEnv)(cmdObj.envPath);
        telemetry_1.default.maybeShowNotice();
        telemetry_1.default.record('command_used', {
            name: 'list prompts',
        });
        await telemetry_1.default.send();
        const prompts = (await (0, util_1.getPrompts)(Number(cmdObj.n) || undefined)).sort((a, b) => b.recentEvalId.localeCompare(a.recentEvalId));
        const tableData = prompts.map((prompt) => ({
            'Prompt ID': prompt.id.slice(0, 6),
            Raw: prompt.prompt.raw.slice(0, 100) + (prompt.prompt.raw.length > 100 ? '...' : ''),
            '# evals': prompt.count,
            'Most recent eval': prompt.recentEvalId.slice(0, 6),
        }));
        logger_1.default.info((0, table_1.wrapTable)(tableData));
        (0, util_1.printBorder)();
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show prompt <id>')} to see details of a specific prompt.`);
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show eval <id>')} to see details of a specific evaluation.`);
    });
    listCommand
        .command('datasets')
        .description('List datasets used')
        .option('--env-path <path>', 'Path to the environment file')
        .option('-n <limit>', 'Number of datasets to display')
        .action(async (cmdObj) => {
        (0, util_1.setupEnv)(cmdObj.envPath);
        telemetry_1.default.maybeShowNotice();
        telemetry_1.default.record('command_used', {
            name: 'list datasets',
        });
        await telemetry_1.default.send();
        const datasets = (await (0, util_1.getTestCases)(Number(cmdObj.n) || undefined)).sort((a, b) => b.recentEvalId.localeCompare(a.recentEvalId));
        const tableData = datasets.map((dataset) => ({
            'Dataset ID': dataset.id.slice(0, 6),
            'Highest scoring prompt': dataset.prompts
                .sort((a, b) => (b.prompt.metrics?.score || 0) - (a.prompt.metrics?.score || 0))[0]
                .id.slice(0, 6),
            '# evals': dataset.count,
            '# prompts': dataset.prompts.length,
            'Most recent eval': dataset.recentEvalId.slice(0, 6),
        }));
        logger_1.default.info((0, table_1.wrapTable)(tableData));
        (0, util_1.printBorder)();
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show dataset <id>')} to see details of a specific dataset.`);
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show prompt <id>')} to see details of a specific prompt.`);
        logger_1.default.info(`Run ${chalk_1.default.green('promptfoo show eval <id>')} to see details of a specific evaluation.`);
    });
}
exports.listCommand = listCommand;
//# sourceMappingURL=list.js.map